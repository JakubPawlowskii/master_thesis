% !TeX root = /home/jakubp/Storage/OneDrive/Documents/Studies/masters/master_thesis/thesis.tex

\chapter{Krylov subspace methods for quantum many-body systems}
\thispagestyle{chapterBeginStyle}

One of the two purposes of this thesis is to develop and test a set of numerical tools based on the Krylov subspace methods,
which is a family of iterative methods concerned with projecting high dimensional problems into smaller dimension subspaces
and solving them therein. The titular Krylov subspace is introduced via
\begin{definition}
	Let \(\vectorbold{v} \in \CC^{m}\) be a vector and \(A\in \CC^{m\cross m}\) be a matrix. A \(k\)-th 
	\textbf{Krylov subspace} is defined as
	\begin{equation*}
		\mathcal{K}_k = \mathrm{span}\{v, Av, A^2v,\ldots,A^{k-1}v \}
	\end{equation*}
\end{definition}
Maximal dimension of a Krylov subspace is bounded from above by \(\mathrm{rank}(A) + 1\)~\autocite{Simoncini2015}.

This chapter serves as a
pedagogical introduction to the core ideas of these methods, including some of the usually omitted mathematical details.
For the initial part of this exposition we follow the excellent textbook of numerical linear algebra by~\textcite{Trefethen1997},
whereas for further applications to quantum many-body physics we rely on the excellent treatments of the topic
found in~\textcite{Sandvik2010} and PhD thesis by~\textcite{Crivelli2016}.

\textcolor{red}{Reproduce Figure 32.1 about the difference between direct and iterative algorithms}

We start this chapter by quickly sketching the problems with "direct" algorithms such as Exact Diagonalization, and quickly
follow with the fundamental iterative algorithm for sparse nonhermitian matrices, the Arnoldi iteration. Its outputs admits several
possible interpretations, however we shall focus on the problem of locating extremal eigenvalues.
Afterwards, we restrict our attention to the class of hermitian matrices, to which of course all typical tigh-binding Hamiltonians
belong to, and describe the Lanczos algorithm, which allows for efficient calculation of the ground state eigenvalue and eigenvector,
and thus the ground state properties of a system.
Yet in this work we are mainly interested in infinite temperature calculations, for which in principle sampling of the whole
spectrum is required. To this end, in subsequent sections we develop a scheme for time evolution of arbitrary state,
called the Krylov propagator~\autocite{Park1986}, and combine it with the idea of Dynamical Quantum Typicality (DQT),
which states that a single pure state can have the same properties as an ensemble density matrix~\autocite{Gemmer2003,Goldstein2006,Popescu2006}.
We finish this chapter with a proposal of employing this method to the identification of local integrals of motion in a given
tight-binding system\autocite{Mierzejewski2015a}.
%  For the remainder of this chapter, let \(H\) denote arbitrary tight-binding Hamiltonian, \(\mathcal{H}\) the associated Hilbert space,
%  and \(\dimension = \textrm{dim}\left(\mathcal{H}\right) < \infty\) its dimension.


 \section{Problems with Exact Diagonalization}
  The most straightforward numerical method for studying discrete quantum many-body systems is without a doubt
 Exact Diagonalization (ED)~\autocite{Weisse2008}. It belongs to the family of the so-called direct algorithms and
allows one to obtain numerically exact set of eigenvalues and eigenvectors and subsequently compute any desired properties
 of the system, be it thermal expectation values, time evolution, Green's functions etc. Unfortunately, the starting point of any
 ED calculation is the expression of the Hamiltonian as a dense matrix, in the Hilbert space basis of choice. Taking into account
 the fact that the dimension many-body Hilbert space grows exponentially with the size of the system, the memory cost quickly becomes
 prohibitive, even when exploiting conservation laws and related symmetries. For example, in the case of a spin chain of length L, with 
 on-site basis dimension being 2, the full dimension of the Hilbert space would be \(\dimension = 2^{L}\). Taking a modest length of 25 sites, that gives
 \(2^{25} = 33554432\approx 3.36 \cdot 10^7\) basis states and a memory footprint of Hamiltonian matrix of around 9PB (using double-precision
 floating point numbers), which is 9000 times more than the typical consumer hard drive capacity of 1TB. Even assuming some kind of distributed
 memory platform allowing for handling such large matrices, the computational complexity of ED, requiring \(O(\dimension^3)\) operations,
 is the next major hurdle. Therefore, it is exceedingly
 difficult to probe the thermodynamic limit physics and ED calculations suffer from finite size effects.
 
 Closer investigation of the Hamiltonian matrix, expressed in computational basis\footnote{For spin systems, it is the eigenbasis of \(\Sz\) operator.}
 quickly reveals the inefficiency of dense storage. Looking at Figure~(\textcolor{red}{Here figure with Hamiltonian, basis ordered by magnetization}), we see that most of
 the matrix elements are zero. In fact only about \(\mu \propto \dimension \) out of \(\dimension^2\) matrix elements
 are non-zero. Hence, a numerical scheme leveraging this sparsity is highly desirable. This is exactly what the Krylov subspace algorithms
 do, by the virtue of requiring only a "black box" computation of matrix-vector product, which can be fairly easily implemented in a way
 requiring only \(O(\mu \dimension)\) operations.

\section{Calculation of extremal eigenvalues}
Our goal in this section is to develop the Lanczos algorithm for ground state search of hermitian matrices, and along
the way understand how and why it works.

\subsection{Arnoldi iteration}
The Lanczos algorithm is special case of a more general algorithm, called Arnoldi iteration, designed to transform
a general, nonhermitian matrix \(A\in \CC^{m\cross m} \) via a orthogonal
\footnote{Orthogonal in this context means that \(Q^{\dagger}Q = I_{m \cross m}\)} similarity transformation to a Hessenberg form \(A = QHQ^{\dagger}\).
Such transformation always exist~\autocite{Garcia2017}.


\begin{definition}
    A square, \(m \cross m\) matrix \(H\) is said to be in upper \textbf{Hessenberg} form if
    \(\forall i,j\in \{1,\ldots,n\}: i > j+1 \implies (A)_{i,j}=0 \).
    It is said to be in \textbf{lower Hessenberg form}, if its transpose is in upper Hessenberg form.
\end{definition}
A Hessenberg matrix differs from a triangular one by one additional super- or subdiagonal.
Such form is desirable, because many numerical algorithms in linear algebra experience considerable speedup
from leveraging triangular structure of a matrix, and sometimes those benefits carry over to this almost-triangular
case. A particularly important strength of the Arnoldi iteration is that it can be interrupted before completion (cf. fig 32.1),
thus producing only an approximation of the Hessenberg form in situation where \(m\) is so large, that
full computations are infeasible (eg. in quantum many-body physics).

Assume now that we are able to only compute the first \(n < m\) columns of the equation \(AQ=QH\).
Let \(Q_n\) be the restriction of \(Q\) to \(n\) columns and let them be denoted by \(\vectorbold{q_1},\vectorbold{q_2}, \ldots 
\vectorbold{ q_n}\).
Denoting by \(\tilde{H}_n\) the \((n+1)\cross n\) upper left section of H, which is also a Hessenberg matrix, we can 
write down the following \(n\)-step approximation to the full decomposition
\begin{equation}
	AQ_{n}=Q_{n+1}\tilde{H}_{n}
	\label{eq:krylov_n_approx}
\end{equation}
From this equation we can deduce an \(n+1\) term recurrence relation for the column \(\vectorbold{q_{n+1}}\), however
it is perhaps best illustrated with a simple example in the first place.

\begin{example}
	Let \(A\in \CC^{3\cross 3}\), \(AQ=QH\) be the Hessenberg decomposition and corresponding matrix elements
	be denoted by lowercase letters. We consider the approximation for \(n = 2\), i.e. \(AQ_2 = Q_3 \tilde{H}_2\).
	On the right hand side
	\begin{align*}
		AQ_2 = 
		\begin{bmatrix}
			a_{11} & a_{12} & a_{13} \\
			a_{21} & a_{22} & a_{23} \\
			a_{31} & a_{32} & a_{33}
			\end{bmatrix}
			\begin{bmatrix}
			q_{11} & q_{12} \\
			q_{21} & q_{22} \\
			q_{31} & q_{32}
			\end{bmatrix}
			=&
			\begin{bmatrix}
			a_{11}q_{11} + a_{12}q_{21} + a_{13}q_{31} & a_{11}q_{12} + a_{12}q_{22} + a_{13}q_{32} \\
			a_{21}q_{11} + a_{22}q_{21} + a_{23}q_{31} & a_{21}q_{12} + a_{22}q_{22} + a_{23}q_{32} \\
			a_{31}q_{11} + a_{32}q_{21} + a_{33}q_{31} & a_{31}q_{12} + a_{32}q_{22} + a_{33}q_{32}
			\end{bmatrix} \\
			=&
			\begin{bmatrix}
				(A\vectorbold{q_1})_1 & (A\vectorbold{q_2})_1 \\
				(A\vectorbold{q_1})_2 & (A\vectorbold{q_2})_2 \\
				(A\vectorbold{q_1})_3 & (A\vectorbold{q_2})_3
			\end{bmatrix}
	\end{align*}
	On the left hand side
	\begin{align*}
		Q_3 H_2 = 
		\begin{bmatrix}
			q_{11} & q_{12} & q_{13} \\
			q_{21} & q_{22} & q_{23} \\
			q_{31} & q_{32} & q_{33}
			\end{bmatrix}
			\begin{bmatrix}
			h_{11} & h_{12} \\
			h_{21} & h_{22} \\
			0 & h_{32}
			\end{bmatrix}
			=&
			\begin{bmatrix}
				q_{11}h_{11} + q_{12}h_{21} & q_{11}h_{12} + q_{12}h_{22} + q_{13}h_{32} \\
				q_{21}h_{11} + q_{22}h_{21} & q_{21}h_{12} + q_{22}h_{22} + q_{23}h_{32}\\
				q_{31}h_{11} + q_{32}h_{21} & q_{31}h_{12} + q_{32}h_{22}+ q_{33}h_{32}
			\end{bmatrix}\\		
			=&
			\begin{bmatrix}
				h_{11}(\vectorbold{q_1})_1 + h_{21}(\vectorbold{q_2})_1 & h_{12}(\vectorbold{q_1})_1 + h_{22}(\vectorbold{q_2})_1 + h_{32}(\vectorbold{q_3})_1 \\
				h_{11}(\vectorbold{q_1})_2 + h_{21}(\vectorbold{q_2})_2 & h_{12}(\vectorbold{q_1})_2 + h_{22}(\vectorbold{q_2})_2 + h_{32}(\vectorbold{q_3})_2 \\
				h_{11}(\vectorbold{q_1})_3 + h_{21}(\vectorbold{q_2})_3 & h_{12}(\vectorbold{q_1})_3 + h_{22}(\vectorbold{q_2})_3 + h_{32}(\vectorbold{q_3})_3
			\end{bmatrix}
	\end{align*}
	From the above calculation and~\ref{eq:krylov_n_approx} we can read off two identities
	\begin{align*}
		A\vectorbold{q_1} &= h_{11}\vectorbold{q_1}+h_{21}\vectorbold{q_2}\\
		A\vectorbold{q_2} &= h_{21}\vectorbold{q_1}+h_{22}\vectorbold{q_2} + h_{32}\vectorbold{q_3}
	\end{align*}
	Therefore we get, assuming \(\vectorbold{q_1}\) is known,
	\begin{align*}
		\vectorbold{q_2} =& \frac{A \vectorbold{q_1} - h_{11} \vectorbold{q_1}}{h_{21}}\\
		\vectorbold{q_3} =& \frac{A \vectorbold{q_2} - h_{21} \vectorbold{q_1} - h_{22}\vectorbold{q_2}}{h_{32}}
	\end{align*}
\end{example}
Generalizing the above example, we arrive at the desired \(n+1\) term recurrence relation for \(\vectorbold{q_{n+1}}\)
\begin{equation}
	\vectorbold{q_{n+1}} = \frac{A \vectorbold{q_n} - \sum_{m=1}^{n} h_{mn}\vectorbold{q_m}}{h_{n+1,n}}
	\label{eq:arnoldi_recurrence}
\end{equation}
We can now easily cast the above recurrence into a pseducode algorithm:
\begin{algorithm}
	\algrenewcommand\algorithmicrequire{\textbf{Input: }}
	\algrenewcommand\algorithmicensure{\textbf{Output: }}
	\caption{Arnoldi iteration}
	\label{alg:arnoldi}
	\begin{algorithmic}[1]
		\Require \(v \in \CC^m\), \(A \in \CC^{m\cross m}\), number of steps \(n\)
		\Ensure columns of \(Q_n\)
		\State \(\vectorbold{q_1} = \vectorbold{v}/\norm*{\vectorbold{v}}\)
		\For{\(i = 1:n-1\)}
			\State \(\vectorbold{q} = A\vectorbold{q_i}\)
			\For{\(j = 1:i\)}
				\State \(h_{ji} = \mathrm{cdot}(\vectorbold{q_j},\vectorbold{q})\) \Comment{\(\mathrm{cdot}\) is the complex dot product on \(\CC^m\).}
				\State \(\vectorbold{q} = \vectorbold{q} - h_{ji}\vectorbold{q_j}\) \Comment{In exact arithmetic, this enusres orthogonality.}
			\EndFor
			\State \(h_{i+1,i} = \norm{\vectorbold{q}} \) 
			\State \(\vectorbold{q_{i+1}} = \vectorbold{q}/h_{i+1,i} \)
		\EndFor
	\end{algorithmic}
\end{algorithm}
Step 9 of the Algorithm~\ref{alg:arnoldi} may be questionable, as we are dividing by a norm of a vector, which
after all can be equal to zero. However, in practical applications of Arnoldi iteration it usually means
that our calculations have converged and the iterations may be stopped.


Examining closely the Arnoldi iteration algorithm, we notice that it is essentially the Gram-Schmidt
procedure applied to the vectors \(\{\vectorbold{v}, A\vectorbold{v},\ldots, A^{n-1}\vectorbold{v}\}\) and hence the 
vectors \(\{\vectorbold{q_1}, \vectorbold{q_2},\ldots,\vectorbold{q_n}\}\) form an orthonormal basis
of the Krylov subspace \(\mathcal{K}_n\). The orthonormality condition is concisely expressed by 
the fact that \(Q^{\dagger}_n Q_{n+1} \) is the \(n\cross (n+1)\) identity matrix. Multiplying the left-hand side of
equation~\eqref{eq:krylov_n_approx} by \(Q^{\dagger}_n\) we get
\begin{equation}
	Q^{\dagger}_n A Q_n = \underbrace{Q^{\dagger}_n Q_{n+1}}_{\mathrm{Id}_{n\cross(n+1)}}\tilde{H}_n = H_n \in \CC^{n\cross n}
\end{equation}
where \(H_n\) is the Hessenberg matrix \(\tilde{H}_n\) with its last row removed. 

To understand the meaning
of matrix \(H_n\) from the point of view of linear algebra, consider the following reasoning. Imagine we are
given an endormophism of the space \(\CC^m\), represented in the standard basis by a matrix \(A\).
We would like to restrict it to a endormophism of the Krylov subspace \(\mathcal{K}_n,\; n < m\). Of course,
as \(\vectorbold{v} \in \mathcal{K}_n \implies \vectorbold{v} \in \CC^m\), we can calculate the action of \(A\) on a vector
from Krylov subspace in a straightforward way. However, the resulting vector \(A\vectorbold{v}\) in not guaranteed
to be an element of \(\mathcal{K}_n\). We need to orthogonally project it back to the subspace. Such projection
is realized by \(Q_n Q^{\dagger}_n \in \CC^{m\cross m}\) and hence, with respect to the standard basis on \(\CC^m\),
the desired restriction can be written as \(Q_n Q^{\dagger}_n A\). Transforming it to the basis given by columns of
\(Q_n\) we get \( Q_n^{-1}\left(Q_n Q^{\dagger}_n A\right)Q_n = Q^{\dagger}_n A Q_n\). Thus, matrix \(H_n\)
is the orthogonal projection of \(A\) to the subspace \(\mathcal{K}_n\), represented in the basis
\(\{\vectorbold{q_1},\vectorbold{q_2},\ldots,\vectorbold{q_n}\}\).

\(H_n\) is once again a square matrix, so we can talk about its eigenvalues %\(\{\lambda_i\}_{i = 1}^n\)
in the usual fashion. These numbers are called the \textit{Arnoldi eigenvalues estimates at step \(n\)}, or
the \textit{Ritz values with respect to \(\mathcal{K}_n\)}.
Given the interpretation above, we may suspect that they would be related to the eigenvalues of the original matrix \(A\).
Indeed, as we shall see in a moment, some of the Ritz values are extremally good approximations of some of the
original eigenvalues.


\subsection{How Arnoldi iteration locates extremal eigenvalues}

By carrying out the Arnoldi iterations for succesive steps, and at each step \(n\) (or at just some of the steps)
calculating the eigenvalues of the Hessenberg matrix \(H_n\), we are left with sequences of Ritz values. Some
of them often converge rapidly to, what we reasonably assume, eigenvalues of the original matrix \(A\).
However in practive, the maximal accessible \(n\) is much smaller than \(m\), so we cannot expect to find
all eigenvalues.


% write about how Arnoldi localizes eigenvalues
% maybe plot my own Arnoldi lemniscates


\subsection{Restriction to hermitian case: Lanczos iteration}



\section{Time evolution via the Krylov propagator}

\section{Physical interlude: Quantum Typicality}

\section{Correlation functions and the search for integrals of motion}



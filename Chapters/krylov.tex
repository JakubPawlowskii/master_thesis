% !TeX root = /home/jakubp/Storage/OneDrive/Documents/Studies/masters/master_thesis/thesis.tex

\chapter{Krylov subspace methods for quantum many-body systems}
\thispagestyle{chapterBeginStyle}

One of the two purposes of this thesis is to develop and test a set of numerical tools based on the Krylov subspace methods,
which is a family of iterative methods concerned with projecting high dimensional problems into smaller dimension subspaces
and solving them therein. The titular Krylov subspace is introduced via
\begin{definition}
	Let \(\vectorbold{v} \in \CC^{m}\) be a vector and \(A\in \CC^{m\cross m}\) be a matrix. A \(k\)-th 
	\textbf{Krylov subspace} is defined as
	\begin{equation*}
		\K{k} := \mathrm{span}\{\,\vectorbold{v}, A\vectorbold{v}, A^2\vectorbold{v},\ldots,A^{k-1}\vectorbold{v} \,\}
	\end{equation*}
	\label{def:krylov}
\end{definition}
Maximal dimension of a Krylov subspace is bounded from above by \(\mathrm{rank}(A) + 1\)~\autocite{Simoncini2015}.

This chapter serves as a
pedagogical introduction to the core ideas of these methods, including some of the usually omitted mathematical details.
For the initial part of this exposition we follow the excellent textbook of numerical linear algebra by~\textcite{Trefethen1997},
whereas for further applications to quantum many-body physics we rely on the excellent treatments of the topic
found in~\textcite{Sandvik2010} and PhD thesis by~\textcite{Crivelli2016}.

\textcolor{red}{Reproduce Figure 32.1 about the difference between direct and iterative algorithms}

We start this chapter by quickly sketching the problems with "direct" algorithms such as Exact Diagonalization, and quickly
follow with the fundamental iterative algorithm for sparse nonhermitian matrices, the Arnoldi iteration. Its outputs admits several
possible interpretations, however we shall focus on the problem of locating extremal eigenvalues.
Afterwards, we restrict our attention to the class of hermitian matrices, to which of course all typical tigh-binding Hamiltonians
belong to, and describe the Lanczos algorithm, which allows for efficient calculation of the ground state eigenvalue and eigenvector,
and thus the ground state properties of a system.
Yet in this work we are mainly interested in infinite temperature calculations, for which in principle sampling of the whole
spectrum is required. To this end, in subsequent sections we develop a scheme for time evolution of arbitrary state,
called the Krylov propagator~\autocite{Park1986}, and combine it with the idea of Dynamical Quantum Typicality (DQT),
which states that a single pure state can have the same properties as an ensemble density matrix~\autocite{Gemmer2003,Goldstein2006,Popescu2006}.
We finish this chapter with a proposal of employing this method to the identification of local integrals of motion in a given
tight-binding system\autocite{Mierzejewski2015a}.
%  For the remainder of this chapter, let \(H\) denote arbitrary tight-binding Hamiltonian, \(\mathcal{H}\) the associated Hilbert space,
%  and \(\dimension = \textrm{dim}\left(\mathcal{H}\right) < \infty\) its dimension.


 \section{Problems with Exact Diagonalization}
  The most straightforward numerical method for studying discrete quantum many-body systems is without a doubt
 Exact Diagonalization (ED)~\autocite{Weisse2008}. It belongs to the family of the so-called direct algorithms and
allows one to obtain numerically exact set of eigenvalues and eigenvectors and subsequently compute any desired properties
 of the system, be it thermal expectation values, time evolution, Green's functions etc. Unfortunately, the starting point of any
 ED calculation is the expression of the Hamiltonian as a dense matrix, in the Hilbert space basis of choice. Taking into account
 the fact that the dimension many-body Hilbert space grows exponentially with the size of the system, the memory cost quickly becomes
 prohibitive, even when exploiting conservation laws and related symmetries. For example, in the case of a spin chain of length L, with 
 on-site basis dimension being 2, the full dimension of the Hilbert space would be \(\dimension = 2^{L}\). Taking a modest length of 25 sites, that gives
 \(2^{25} = 33554432\approx 3.36 \cdot 10^7\) basis states and a memory footprint of Hamiltonian matrix of around 9PB (using double-precision
 floating point numbers), which is 9000 times more than the typical consumer hard drive capacity of 1TB. Even assuming some kind of distributed
 memory platform allowing for handling such large matrices, the computational complexity of ED, requiring \(O(\dimension^3)\) operations,
 is the next major hurdle. Therefore, it is exceedingly
 difficult to probe the thermodynamic limit physics and ED calculations suffer from finite size effects.
 
 Closer investigation of the Hamiltonian matrix, expressed in computational basis\footnote{For spin systems, it is the eigenbasis of \(\Sz\) operator.}
 quickly reveals the inefficiency of dense storage. Looking at Figure~(\textcolor{red}{Here figure with Hamiltonian, basis ordered by magnetization}), we see that most of
 the matrix elements are zero. In fact only about \(\mu \propto \dimension \) out of \(\dimension^2\) matrix elements
 are non-zero. Hence, a numerical scheme leveraging this sparsity is highly desirable. This is exactly what the Krylov subspace algorithms
 do, by the virtue of requiring only a "black box" computation of matrix-vector product, which can be fairly easily implemented in a way
 requiring only \(O(\mu \dimension)\) operations.

\section{Calculation of extremal eigenvalues}
Our goal in this section is to develop the Lanczos algorithm for ground state search of hermitian matrices, and along
the way understand how and why it works.

\subsection{Arnoldi iteration}
The Lanczos algorithm is special case of a more general algorithm, called Arnoldi iteration, designed to transform
a general, nonhermitian matrix \(A\in \CC^{m\cross m} \) via a orthogonal
\footnote{Orthogonal in this context means that \(Q^{\dagger}Q = I_{m \cross m}\)} similarity transformation to a Hessenberg form \(A = QHQ^{\dagger}\).
Such transformation always exist~\autocite{Garcia2017}.


\begin{definition}
    A square, \(m \cross m\) matrix \(H\) is said to be in upper \textbf{Hessenberg} form if
    \(\forall i,j\in \{\,1,\ldots,n\,\}: i > j+1 \implies (A)_{i,j}=0 \).
    It is said to be in \textbf{lower Hessenberg form}, if its transpose is in upper Hessenberg form.
\end{definition}
A Hessenberg matrix differs from a triangular one by one additional super- or subdiagonal.
Such form is desirable, because many numerical algorithms in linear algebra experience considerable speedup
from leveraging triangular structure of a matrix, and sometimes those benefits carry over to this almost-triangular
case. A particularly important strength of the Arnoldi iteration is that it can be interrupted before completion (cf. fig 32.1),
thus producing only an approximation of the Hessenberg form in situation where \(m\) is so large, that
full computations are infeasible (eg. in quantum many-body physics).

Assume now that we are able to only compute the first \(n < m\) columns of the equation \(AQ=QH\).
Let \(Q_n\) be the restriction of \(Q\) to \(n\) columns and let them be denoted by \(\vectorbold{q_1},\vectorbold{q_2}, \ldots 
\vectorbold{ q_n}\in \CC^m\).
Denoting by \(\tilde{H}_n\) the \((n+1)\cross n\) upper left section of H, which is also a Hessenberg matrix, we can 
write down the following \(n\)-step approximation to the full decomposition
\begin{equation}
	AQ_{n}=Q_{n+1}\tilde{H}_{n}
	\label{eq:krylov_n_approx}
\end{equation}
From this equation we can deduce an \(n+1\) term recurrence relation for the column \(\vectorbold{q_{n+1}}\), however
it is perhaps best illustrated with a simple example in the first place.

\begin{example}
	Let \(A\in \CC^{3\cross 3}\), \(AQ=QH\) be the Hessenberg decomposition and corresponding matrix elements
	be denoted by lowercase letters. We consider the approximation for \(n = 2\), i.e. \(AQ_2 = Q_3 \tilde{H}_2\).
	On the right hand side
	\begin{align*}
		AQ_2 = 
		\begin{bmatrix}
			a_{11} & a_{12} & a_{13} \\
			a_{21} & a_{22} & a_{23} \\
			a_{31} & a_{32} & a_{33}
			\end{bmatrix}
			\begin{bmatrix}
			q_{11} & q_{12} \\
			q_{21} & q_{22} \\
			q_{31} & q_{32}
			\end{bmatrix}
			&=
			\begin{bmatrix}
			a_{11}q_{11} + a_{12}q_{21} + a_{13}q_{31} & a_{11}q_{12} + a_{12}q_{22} + a_{13}q_{32} \\
			a_{21}q_{11} + a_{22}q_{21} + a_{23}q_{31} & a_{21}q_{12} + a_{22}q_{22} + a_{23}q_{32} \\
			a_{31}q_{11} + a_{32}q_{21} + a_{33}q_{31} & a_{31}q_{12} + a_{32}q_{22} + a_{33}q_{32}
			\end{bmatrix} \\
			&=
			\begin{bmatrix}
				(A\vectorbold{q_1})_1 & (A\vectorbold{q_2})_1 \\
				(A\vectorbold{q_1})_2 & (A\vectorbold{q_2})_2 \\
				(A\vectorbold{q_1})_3 & (A\vectorbold{q_2})_3
			\end{bmatrix}
	\end{align*}
	On the left hand side
	\begin{align*}
		Q_3 H_2 = 
		\begin{bmatrix}
			q_{11} & q_{12} & q_{13} \\
			q_{21} & q_{22} & q_{23} \\
			q_{31} & q_{32} & q_{33}
			\end{bmatrix}
			\begin{bmatrix}
			h_{11} & h_{12} \\
			h_{21} & h_{22} \\
			0 & h_{32}
			\end{bmatrix}
			&=
			\begin{bmatrix}
				q_{11}h_{11} + q_{12}h_{21} & q_{11}h_{12} + q_{12}h_{22} + q_{13}h_{32} \\
				q_{21}h_{11} + q_{22}h_{21} & q_{21}h_{12} + q_{22}h_{22} + q_{23}h_{32}\\
				q_{31}h_{11} + q_{32}h_{21} & q_{31}h_{12} + q_{32}h_{22}+ q_{33}h_{32}
			\end{bmatrix}\\		
			&=
			\begin{bmatrix}
				h_{11}(\vectorbold{q_1})_1 + h_{21}(\vectorbold{q_2})_1 & h_{12}(\vectorbold{q_1})_1 + h_{22}(\vectorbold{q_2})_1 + h_{32}(\vectorbold{q_3})_1 \\
				h_{11}(\vectorbold{q_1})_2 + h_{21}(\vectorbold{q_2})_2 & h_{12}(\vectorbold{q_1})_2 + h_{22}(\vectorbold{q_2})_2 + h_{32}(\vectorbold{q_3})_2 \\
				h_{11}(\vectorbold{q_1})_3 + h_{21}(\vectorbold{q_2})_3 & h_{12}(\vectorbold{q_1})_3 + h_{22}(\vectorbold{q_2})_3 + h_{32}(\vectorbold{q_3})_3
			\end{bmatrix}
	\end{align*}
	From the above calculation and~\ref{eq:krylov_n_approx} we can read off two identities
	\begin{align*}
		A\vectorbold{q_1} &= h_{11}\vectorbold{q_1}+h_{21}\vectorbold{q_2}\\
		A\vectorbold{q_2} &= h_{21}\vectorbold{q_1}+h_{22}\vectorbold{q_2} + h_{32}\vectorbold{q_3}
	\end{align*}
	Therefore we get, assuming \(\vectorbold{q_1}\) is known,
	\begin{align*}
		\vectorbold{q_2} &= \frac{A \vectorbold{q_1} - h_{11} \vectorbold{q_1}}{h_{21}}\\
		\vectorbold{q_3} &= \frac{A \vectorbold{q_2} - h_{21} \vectorbold{q_1} - h_{22}\vectorbold{q_2}}{h_{32}}
	\end{align*}
\end{example}
Generalizing the above example, we arrive at the desired \(n+1\) term recurrence relation for \(\vectorbold{q_{n+1}}\)
\begin{equation}
	\vectorbold{q_{n+1}} = \frac{A \vectorbold{q_n} - \sum_{m=1}^{n} h_{mn}\vectorbold{q_m}}{h_{n+1,n}}
	\label{eq:arnoldi_recurrence}
\end{equation}
We can now easily cast the above recurrence into a pseducode algorithm:
\begin{algorithm}
	\algrenewcommand\algorithmicrequire{\textbf{Input: }}
	\algrenewcommand\algorithmicensure{\textbf{Output: }}
	\caption{Arnoldi iteration}
	\label{alg:arnoldi}
	\begin{algorithmic}[1]
		\Require \(v \in \CC^m\), \(A \in \CC^{m\cross m}\), number of steps \(n\)
		\Ensure columns of \(Q_n\)
		\State \(\vectorbold{q_1} = \vectorbold{v}/\norm*{\vectorbold{v}}\)
		\For{\(i = 1:n-1\)}
			\State \(\vectorbold{q} = A\vectorbold{q_i}\)
			\For{\(j = 1:i\)}
				\State \(h_{ji} = \mathrm{cdot}(\vectorbold{q_j},\vectorbold{q})\) \Comment{\(\mathrm{cdot}\) is the complex dot product on \(\CC^m\).}
				\State \(\vectorbold{q} = \vectorbold{q} - h_{ji}\vectorbold{q_j}\) \Comment{In exact arithmetic, this enusres orthogonality.}
			\EndFor
			\State \(h_{i+1,i} = \norm{\vectorbold{q}} \) 
			\State \(\vectorbold{q_{i+1}} = \vectorbold{q}/h_{i+1,i} \)
		\EndFor
	\end{algorithmic}
\end{algorithm}
Step 9 of the Algorithm~\ref{alg:arnoldi} may be questionable, as we are dividing by a norm of a vector, which
after all can be equal to zero. However, in practical applications of Arnoldi iteration it usually means
that our calculations have converged and the iterations may be stopped.


Examining closely the Arnoldi iteration algorithm, we notice that it is essentially the Gram-Schmidt
procedure applied to the vectors \(\{\,\vectorbold{v}, A\vectorbold{v},\ldots, A^{n-1}\vectorbold{v}\,\}\) and hence the 
vectors \(\{\,\vectorbold{q_1}, \vectorbold{q_2},\ldots,\vectorbold{q_n}\,\}\) form an orthonormal basis
of the Krylov subspace \(\K{n}\). The orthonormality condition is concisely expressed by 
the fact that \(Q^{\dagger}_n Q_{n+1} \) is the \(n\cross (n+1)\) identity matrix. Multiplying the left-hand side of
equation~\eqref{eq:krylov_n_approx} by \(Q^{\dagger}_n\) we get
\begin{equation}
	Q^{\dagger}_n A Q_n = \underbrace{Q^{\dagger}_n Q_{n+1}}_{\mathrm{Id}_{n\cross(n+1)}}\tilde{H}_n = H_n \in \CC^{n\cross n}
	\label{eq:hessenberg}
\end{equation}
where \(H_n\) is the Hessenberg matrix \(\tilde{H}_n\) with its last row removed. 

To understand the meaning
of matrix \(H_n\) from the point of view of linear algebra, consider the following reasoning. Imagine we are
given an endormophism of the space \(\CC^m\), represented in the standard basis by a matrix \(A\).
We would like to restrict it to a endormophism of the Krylov subspace \(\K{n},\; n < m\). Of course,
as \(\vectorbold{q} \in \K{n} \implies \vectorbold{q} \in \CC^m\), we can calculate the action of \(A\) on a vector
from Krylov subspace in a straightforward way. However, the resulting vector \(A\vectorbold{q}\) in not guaranteed
to be an element of \(\K{n}\). We need to orthogonally project it back to the subspace. Such projection
is realized by \(Q_n Q^{\dagger}_n \in \CC^{m\cross m}\) and hence, with respect to the standard basis on \(\CC^m\),
the desired restriction can be written as \(Q_n Q^{\dagger}_n A\). Transforming it to the basis given by columns of
\(Q_n\) we get \( Q_n^{-1}\left(Q_n Q^{\dagger}_n A\right)Q_n = Q^{\dagger}_n A Q_n\). Thus, matrix \(H_n\)
is the orthogonal projection of \(A\) to the subspace \(\K{n}\), represented in the basis
\(\{\,\vectorbold{q_1},\vectorbold{q_2},\ldots,\vectorbold{q_n}\,\}\).

\(H_n\) is once again a square matrix, so we can talk about its eigenvalues \(\{\theta_i\}_{i = 1}^n\)
in the usual fashion. These numbers are called the \textit{Arnoldi eigenvalues estimates at step \(n\)}, or
the \textit{Ritz values with respect to \(\K{n}\)}.
Given the interpretation above, we may suspect that they would be related to the eigenvalues of the original matrix \(A\).
Indeed, as we shall see in a moment, some of the Ritz values are extremally good approximations of some of the
original eigenvalues.


\subsection{Polynomial approximation and eigenvalues}

By carrying out the Arnoldi iterations for succesive steps, and at each step \(n\) (or at just some of the steps)
calculating the eigenvalues of the Hessenberg matrix \(H_n\), we are left with sequences of Ritz values. Some
of them often converge rapidly to, what we reasonably assume, eigenvalues of the original matrix \(A\).
However in practive, the maximal accessible \(n\) is much smaller than \(m\), so we cannot expect to find
all eigenvalues. As it turns out, Arnoldi iteration typically finds extremal eigenvalues, which fortunately
are those that we are interested in.

Before we will understand the details, let us introduce different, seemingly unrelated problem of 
\textit{polynomial approximation}. We can take any \(\vectorbold{q} \in \K{n}\) and using the 
defining basis of Krylov subspace \(\K{n}\)(Definition~\ref{def:krylov}), expand is as
\begin{align*}
	\vectorbold{q} &= a_0 \vectorbold{v} + a_1 A \vectorbold{v} + a_2 A^2 \vectorbold{v} + \ldots + a_{n-1} A^{n-1} \vectorbold{v} \nonumber \\
				   &= \left(a_0\Id + a_1 A + a_2 A^2 + \ldots + a_{n-1} A^{n-1}\right) \vectorbold{v}
\end{align*}
Utilizing the special structure of vectors from \(\K{n}\), we can define a polynomial \(p(z) = a_0
+ a_1 z + a_2 z^2 + \ldots + a_{n-1}z^{n-1}\), and concisely write our vector as \(\vectorbold{q} = p(A)\vectorbold{v}\).
As the vector \(\vectorbold{q}\) was arbitrary, we have estabilished an isomorphism between the \(n\)-th Krylov
subspace and the space of complex polynomials of maximal degree \(n-1\). We are now ready to state the problem:
\vspace{-0.3cm}
\begin{titled-frame}{Arnoldi Approximation Problem}
	\centering
Given a matrix \(A\in\CC^{m\cross m}\) and a vector \(\vectorbold{v}\in \CC^m\), find  
	\( p \in P^n := \{\, a_0 + a_1 z + \ldots + a_{n-1} z^{n-1} + z^n \mid a_0,a_1,\ldots a_{n-1}\in \CC \,\} \)\\
	such that \(\norm{p(A)\vectorbold{v}}_2\) is minimized.
\end{titled-frame}

Remarkably, the Arnoldi approximation is the exact solution to this problem. This fact is interesting enough that
we state it here as a theorem and, following~\textcite{Trefethen1997}, provide a complete proof.

\begin{theorem}
	If \(\textrm{dim}\left(\K{n}\right) = n\), i.e. matrix having columns \(\vectorbold{v},A\vectorbold{v},\ldots, A^{n-1}
	\vectorbold{v}\) is of rank \(n\), then the Arnoldi Approximation Problem has a unique solution \(p^n\in P^n\),
	given by the characteristic polynomial of the matrix \(H_n\), defined by~\eqref{eq:hessenberg}.
	\label{theorem:arnoldi_approximation}
\end{theorem}

\begin{proof}
	We start with an observation, that given a polynomial \(p\in P^n\), the vector \(p(A)\vectorbold{v}\)
	can be written as \(p(A)\vectorbold{v} = A^n \vectorbold{v} - Q_n \vectorbold{r}\) for some \(\vectorbold{r}\in \CC^n\).
	To see that, note that \(\left(A^n \vectorbold{v} - p(A)\vectorbold{v}\right) \in \K{n}\) and columns of \(Q_n\) form an
	orthonormal basis of \(\K{n}\). Now, we can recast our problem into a slighlty different language, namely finding a vector
	in \(\K{n}\) that is the closest in the sense of \(L_2\) norm to \(A^n \vectorbold{v}\). In short:
	\begin{equation*}
		\vectorbold{r}^{\ast} = \min_{\vectorbold{r}\in\CC^n}\norm*{A^n \vectorbold{v} - Q_n \vectorbold{r}}
	\end{equation*}
	To achieve that, we need to have \(p(A)\vectorbold{v}\perp \K{n}\), that is \(p(A)\vectorbold{v}\) must
	be orthogonal to all basis vectors spanning \(\K{n}\). This is consisely expressed as 
	\(Q^{\dagger}_{n} p(A)\vectorbold{v} = \vectorbold{0}\in \CC^n\).

	Now, we know that the Hessenberg factorization \(A = QHQ^{\dagger}\) exists, and is approximated by \(n\) steps
	of the Arnoldi iteration. Thus, the matrices \(Q\) and \(H\) can have the following block structure:
	\begin{equation}
		Q=\left[\begin{array}{ll}
			Q_n & V
			\end{array}\right],
			\quad H=\left[\begin{array}{cc}
			H_n & 0_{n\cross(m-n)} \\
			Y & 0_{(m-n)\cross(m-n)}
			\end{array}\right]
			\label{eq:block_matrices}
	\end{equation}
	where \(V \in \CC^{m\cross(m-n)}\) is a matrix with orthonormal columns, which are also orthogonal to columns of
	\(Q_n\), and matrix \(Y\in\CC^{(m-n)\cross n}\) has only the upper-right entry different from zero
	(the one from the last row of \(\tilde{H}_n\)). Using the Hessenberg factorization we can write our condition
	as \(Q_n^{\dagger} Q p(H) Q^{\dagger}\vectorbold{v} = \vectorbold{0}\), and because equation~\eqref{eq:block_matrices}
	introduces partitions into \textit{conformable} blocks, we can use the rules of block-matrix algebra to simiplify
	it further~\autocite{Eves1980}.

	First, let us investigare closely the structure of \(p(H)\). We observe that
	\begin{align*}
		H^2 &= \left[\begin{array}{cc}
			H_n & 0_{n\cross(m-n)} \\
			Y & 0_{(m-n)\cross(m-n)}
			\end{array}\right]^2 = 
			\left[\begin{array}{cc}
				H_n^2 & 0_{n\cross(m-n)} \\
				Y H_n & 0_{(m-n)\cross(m-n)}
			\end{array}\right]\\
		H^3 &=
			\left[\begin{array}{cc}
				H_n & 0_{n\cross(m-n)} \\
				Y & 0_{(m-n)\cross(m-n)}
			\end{array}\right]^3 = 
			\left[\begin{array}{cc}
				H_n^3 & 0_{n\cross(m-n)} \\
				Y H_n^2 & 0_{(m-n)\cross(m-n)}
			\end{array}\right]\\
			\ldots\\
		H^n &= 			\left[\begin{array}{cc}
			H_n & 0_{n\cross(m-n)} \\
			Y & 0_{(m-n)\cross(m-n)}
		\end{array}\right]^n = 
		\left[\begin{array}{cc}
			H_n^n & 	  0_{n\cross(m-n)} \\
			Y H_n^{n-1} & 0_{(m-n)\cross(m-n)}
		\end{array}\right]\\
	\end{align*}
Thus \(p(H)\) can be written as
\begin{align*}
	p(H) &= a_0 \Id + a_1 H + a_2 H^2 + \ldots + a_{n-1} H^{n-1} + H^n \\
		&= \left[\begin{array}{cc}
			a_0 \Id + a_1 H_n + a_2 H_n^2 + \ldots + a_{n-1} H_n^{n-1} + H_n^n & 0_{n\cross(m-n)}\\
			a_0 \Id + a_1 Y + a_2 Y H_n + \ldots + a_{n-1} Y H_n^{n-2} + Y H_n^{n-1} & 0_{(m-n)\cross(m-n)}
		\end{array}\right]\\
		&= \left[\begin{array}{cc}
			p(H_n) & 0\\
			\tilde{Y} & 0
		\end{array}\right]
\end{align*}
We have now all the pieces to simplify the orthogonality condition:
\begin{align*}
	\vectorbold{0} &= Q_n^{\dagger} Q p(H) Q^{\dagger}\vectorbold{v}\\
	&= \left[Q_n^{\dagger}\right] 
	\left[\begin{array}{cc}
		Q_n & V
	\end{array}\right]
	\left[\begin{array}{cc}
		p(H_n) & 0_{n\cross(m-n)}\\
		\tilde{Y} & 0_{(m-n)\cross(m-n)}
	\end{array}\right]
	\left[\begin{array}{cc}
		Q_n^{\dagger}\\
		U^{\dagger}
	\end{array}\right] \vectorbold{v}\\
	&= \left[\begin{array}{cc}
		\Id_{n\cross n} & 0_{n\cross(m-n)}
	\end{array}\right] 
	\left[\begin{array}{cc}
		p(H_n) & 0_{n\cross(m-n)}\\
		\tilde{Y} & 0_{(m-n)\cross(m-n)}
	\end{array}\right]
	\left[\begin{array}{cc}
		Q_n^{\dagger}\\
		U^{\dagger}
	\end{array}\right] \vectorbold{v}\\
	&= \left[\begin{array}{cc}
		p(H_n) & 0_{n\cross (m-n)} 
	\end{array}\right]
	\left[\begin{array}{cc}
		Q_n^{\dagger}\\
		U^{\dagger}
	\end{array}\right] \vectorbold{v}\\
	&= p(H_n)Q_n^{\dagger} \vectorbold{v}
\end{align*}
As a final step, notice that by construction the first row of \(Q_n^{\dagger}\) is
\(\vectorbold{v}/\norm{\vectorbold{v}}\), and all the remaining rows are orthogonal to \(\vectorbold{v}\),
therefore only the first column of \(H_n\), or the first \(n\) elements of the first column of \(H\) are required
to be \(0\). By Caylel-Hamilton theorem, this is guaranteed if we take \(p = p^n\), where \(p^n\) is the characteristic
polynomial of \(H_n\). For the uniqueness part, suppose that there exists another polynomial, say \(q^n\) such
that \(q^n \perp \K{n}\). But then \(p^n - q^n\) is a nonzero polynomial of degree \(n-1\) (because \(p^n,\;q^n\) are
monic) such that \((p^n-q^n)(A)\vectorbold{v}=\vectorbold{0}\), and hence vectors \(\vectorbold{v},A\vectorbold{v},\ldots, A^{n-1}
\vectorbold{v}\) are linearly dependent, which violates assumption that \(\textrm{dim}\left(\K{n}\right) = n\).
\end{proof}
This theorem allows us to interpret the Arnoldi eigenvalues estimates \(\{\theta_i\}\) as the roots of the optimal
polynomial. Following the above proof, it is relatively easy to see that they are scale invariant, i.e.
if \(A\to\alpha A\) for some \(\alpha\in\CC\), then \(\{\theta_i\}_{i=1}^n \to \{\alpha\theta_i\}_{i=1}^n\)
and invariant under unitary transformations, i.e. if \(A\to UAU^{\dagger}\) and \(\vectorbold{v}\to U\vectorbold{v}\) for
some unitary \(U\), then the Arnoldi estimates are unchanged. Furthermore, owing to the properties of monic polynomials,
they are also translationally invariant, namely if \(A\to A + \alpha \Id\) for some \(\alpha\in\CC\), then
\(\{\theta_i\}_{i=1}^n \to \{\theta_i + \alpha\}_{i=1}^n\).

In the end we see that the direct purpose of Arnoldi iteration is to solve a polynomial approximation problem
and not to find eigenvalues. However, those two problems have enough in common, that the Arnoldi iteration
produces some correct eigenvalues as a `by-product'. We can reason along the following lines. If our task
is to find a polynomial \(p\in P^n\) minimizing \(\norm{p(A)}\), it may be a good idea to select a polynomial
that has roots close to the eigenvalues of \(A\).  In an extreme situation, when there exists a diagonalization
of \(A\) and it posses only \(n\ll m\) distinct eigenvalues, the minimal polynomial\footnote{A minimal polynomial
of matrix \(A\) is a polynomial \(p\) of the smallest degree such that \(p(A) = 0\). It always divides the
characteristic polynomial.} will coincide with the characteristic polynomial computed via Arnoldi iteration
after \(n\) steps and the Arnoldi eigenvalue approximations will be exact,
provided we start from \(\vectorbold{v}\) having nonzero overlap with all eigenvectors of \(A\).
In most practical situations however, the agreement is only approximate, namely Arnoldi eigenvalues are close to real
eigenvalues, and computed polynomial is such that \(\norm{p(A)}\) is small.

There is more to this story than we have told here, particularly a nice geometric interpreation
of Algorithm~\ref{alg:arnoldi} via \textit{Arnoldi lemniscates}, which illustrates why extremal eigenvalues
are found first, however we shall not concern ourselves with those
matters any further. Interested readers are once again referred to~\textcite{Trefethen1997},
whereas we turn our attention to the case of utmost interest in quantum mechanics, namely Arnoldi iteration
for hermitian matrices.

\subsection{Restriction to hermitian matrices: Lanczos iteration}
From this point onwards, let us switch to the bracket notation favored by physicsts,
 namely \(\ket{v} \equiv \vectorbold{v}\).


\section{Time evolution via the Krylov propagator}

\section{Physical interlude: Quantum Typicality}

\section{Correlation functions and the search for integrals of motion}


